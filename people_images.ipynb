{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "#image analysis: first install dlib in your ada environment (1: conda activate ada , 2: install dlib)\n",
    "import dlib \n",
    "#face recognition library, first install it in your ada environment (1: conda activate ada, 2: install face_recognition)\n",
    "import face_recognition\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data of our additionnal dataset and drop the duplicates\n",
    "actors = pd.DataFrame(pd.read_csv('data/our_datasets/actors_with_tropes.csv'))\n",
    "actors = actors.drop_duplicates(subset='ActorName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iza's personal API key for the TMDB dataset\n",
    "api_key = 'ef0f1f544778e0d8be2e944770018c66'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(api_key, actor):\n",
    "# get the image of the actor from 'the movie database'\n",
    "# Args: api_key: iza's API key for the dataset; actor: name of the actor whose image we want\n",
    "# Returns: image_url: url of the image, images_height, images_width\n",
    "    api_key = api_key\n",
    "    actor = actor\n",
    "    url = 'https://api.themoviedb.org/3/search/person'\n",
    "\n",
    "    params = {\n",
    "    'api_key': api_key,\n",
    "    'query': actor\n",
    "}\n",
    "\n",
    "    #get actor ID:\n",
    "    request = requests.get(url, params=params)\n",
    "    if request.status_code == 200:\n",
    "        json = request.json()\n",
    "\n",
    "        if 'results' in json and json['results']: \n",
    "            person_id =json['results'][0]['id']\n",
    "            url_image= f'https://api.themoviedb.org/3/person/{person_id}/images?api_key={api_key}'\n",
    "            images_json = requests.get(url_image).json()\n",
    "\n",
    "            if 'profiles' in images_json and images_json['profiles']:\n",
    "                images_data = images_json['profiles'][0]['file_path']\n",
    "                images_height = images_json['profiles'][0]['height']\n",
    "                images_width = images_json['profiles'][0]['width']\n",
    "                url_base = 'https://image.tmdb.org/t/p/original'\n",
    "                image_url = f'{url_base}{images_data}'\n",
    "            else:\n",
    "                image_url = '-'\n",
    "                images_height = 'Nan'\n",
    "                images_width = 'Nan'\n",
    "        else:\n",
    "            image_url = '-'\n",
    "            images_height = 'Nan'\n",
    "            images_width = 'Nan'\n",
    "\n",
    "    else:\n",
    "        image_url = '-'\n",
    "        images_height = 'Nan'\n",
    "        images_width = 'Nan'\n",
    "    return image_url,images_height,images_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_images ={'Actor': [], 'Image URL': [], 'Image height':[], 'Image width': []}\n",
    "for actor in actors['ActorName']:\n",
    "    actor_images['Actor'].append(actor)\n",
    "    actor_images['Image URL'].append(get_image(api_key, actor)[0])\n",
    "    actor_images['Image height'].append(get_image(api_key, actor)[1])\n",
    "    actor_images['Image width'].append(get_image(api_key, actor)[2])\n",
    "\n",
    "actor_images_df = pd.DataFrame(actor_images)\n",
    "actor_images_df.to_csv('actor_images', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads \"actor_images.csv\" file\n",
    "actor_images = pd.read_csv('data/our_datasets/actor_images.csv')\n",
    "actor_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding missing values => no URLs \n",
    "missing = actor_images == '-'\n",
    "missing_images = actor_images[missing.any(axis=1)]\n",
    "print(missing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actor_images without the missing urls\n",
    "actor_cleaned = actor_images[~missing.any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmarks(row):\n",
    "\n",
    "\n",
    "    actor_name = row['Actor']\n",
    "    image_url = row['Image URL']\n",
    "\n",
    "    try:\n",
    "        # Retrieve image URL\n",
    "        img = requests.get(image_url)\n",
    "        image_content = BytesIO(img.content)\n",
    "\n",
    "        # Transform the image into an array\n",
    "        img_array = np.asarray(Image.open(image_content))\n",
    "\n",
    "        # Check if the image is black and white (grayscale)\n",
    "        if len(img_array.shape) == 2:  # Grayscale image\n",
    "            # Convert black and white image to RGB\n",
    "            img_array_rgb = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
    "        elif len(img_array.shape) == 3 and img_array.shape[2] == 3:  # RGB image\n",
    "            img_array_rgb = img_array\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image format\")\n",
    "\n",
    "        # Extract facial landmarks\n",
    "        face_landmarks_list = face_recognition.face_landmarks(img_array_rgb)\n",
    "        \n",
    "        if face_landmarks_list:\n",
    "            # Extract facial encodings\n",
    "            face_encodings_list = face_recognition.face_encodings(img_array_rgb)\n",
    "                                                                  \n",
    "        else:\n",
    "            face_encodings_list = np.nan\n",
    "\n",
    "    except Exception as e:\n",
    "        face_landmarks_list = np.nan\n",
    "        face_encodings_list = np.nan\n",
    "        \n",
    "    return pd.Series({'Actor': actor_name, 'Landmarks': face_landmarks_list, 'Encodings':face_encodings_list})\n",
    "\n",
    "\n",
    "    \n",
    "face_encodings2 = actor_cleaned.apply(lambda row: landmarks(row), axis=1)\n",
    "face_encodings2.to_csv('data/our_datasets/actor_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
